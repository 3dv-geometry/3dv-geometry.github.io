[{"layout":"default","title":"Instruction for adding a paper","content":"# Instruction for adding a paper\n\n## Welcome to visit us at 3dv-geometry.github.io\n<img src=\"./hello.jpeg\"  />\n\n\n## Use search engine to find favorate topics\n<img src=\"./search.jpeg\"  />\n\n\n## Find a research paper on the left sidebar \n<img src=\"./example_a.jpeg\"  />\n\n\n## Leave your comments \nYou can put image, equations and other non-text comments here. Check Github Markdown for more details.\n<img src=\"./example_b.jpeg\"  />\n\n\n## Add a research paper to this site\nFirst, you need to be added as a collaborator.\nYou will use .md file, instead of html file, to prepare a paper page.\n<img src=\"./add_file_c.jpeg\"  />\n\n.md file\n\n```md\n# DISN: Deep Implicit Surface Network for High-quality Single-view 3D Reconstruction\n\nQiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomir Mech, Ulrich Neumann\n\nNIPS 2019 [PDF][PDF]\\|[Git][Git]\n\n<img src=\"./disn-result.png\"  />\nReconstructing 3D shapes from single-view images has been a long-standing research problem. In this paper, we present DISN, a Deep Implicit Surface Network which can generate a high-quality detail-rich 3D mesh from a 2D image by predicting the underlying signed distance fields. In addition to utilizing global image features, DISN predicts the projected location for each 3D point on the 2D image and extracts local features from the image feature maps. Combining global and local features significantly improves the accuracy of the signed distance field prediction, especially for the detail-rich areas. To the best of our knowledge, DISN is the first method that constantly captures details such as holes and thin structures present in 3D shapes from single-view images. DISN achieves the state-of-the-art single-view reconstruction performance on a variety of shape categories reconstructed from both synthetic and real images.\n\n## Network\n<img src=\"./disn-network.png\"  />\n\n[Git]:https://github.com/laughtervv/DISN\n[PDF]:https://arxiv.org/abs/1904.04290\n```\n\nRemember to put the page in the *right* location. This is an example of how to organizing the papers.\n<img src=\"./add_file_b.jpeg\"  />\n\n\n\n\n\n","dir":"/HowToUse/","name":"HowToUse.md","path":"HowToUse/HowToUse.md","url":"/HowToUse/HowToUse.html"},{"layout":"default","title":"file1","content":"# file1\n\nsource: `{{ page.path }}`\n","dir":"/3DV/Localization/","name":"file1.md","path":"3DV/Localization/file1.md","url":"/3DV/Localization/file1.html"},{"layout":"default","title":"file2","content":"# file2\n\nsource: `{{ page.path }}`\n","dir":"/3DV/Localization/","name":"file2.md","path":"3DV/Localization/file2.md","url":"/3DV/Localization/file2.html"},{"layout":"default","title":"file3","content":"# file3\n\nsource: `{{ page.path }}`\n","dir":"/3DV/Localization/","name":"file3.md","path":"3DV/Localization/file3.md","url":"/3DV/Localization/file3.html"},{"permalink":"/3DV/Localization/folder2/","layout":"default","title":"I’m folder2","content":"# I'm folder2\n\nsource: `{{ page.path }}`\n","dir":"/3DV/Localization/folder2/","name":"README.md","path":"3DV/Localization/folder2/README.md","url":"/3DV/Localization/folder2/"},{"layout":"default","title":"file1","content":"# file1\n\nsource: `{{ page.path }}`\n","dir":"/3DV/Localization/folder2/","name":"file1.md","path":"3DV/Localization/folder2/file1.md","url":"/3DV/Localization/folder2/file1.html"},{"layout":"default","title":"file2","content":"# file2\n\nsource: `{{ page.path }}`\n","dir":"/3DV/Localization/folder2/","name":"file2.md","path":"3DV/Localization/folder2/file2.md","url":"/3DV/Localization/folder2/file2.html"},{"layout":"default","title":"file3","content":"# file3\n\nsource: `{{ page.path }}`\n","dir":"/3DV/Localization/folder2/","name":"file3.md","path":"3DV/Localization/folder2/file3.md","url":"/3DV/Localization/folder2/file3.html"},{"permalink":"/3DV/Localization/folder2/folder1/","layout":"default","title":"I’m folder1","content":"# I'm folder1\n\nsource: `{{ page.path }}`\n","dir":"/3DV/Localization/folder2/folder1/","name":"README.md","path":"3DV/Localization/folder2/folder1/README.md","url":"/3DV/Localization/folder2/folder1/"},{"layout":"default","title":"file1","content":"# file1\n\nsource: `{{ page.path }}`\n","dir":"/3DV/Localization/folder2/folder1/","name":"file1.md","path":"3DV/Localization/folder2/folder1/file1.md","url":"/3DV/Localization/folder2/folder1/file1.html"},{"layout":"default","title":"file2","content":"# file2\n\nsource: `{{ page.path }}`\n","dir":"/3DV/Localization/folder2/folder1/","name":"file2.md","path":"3DV/Localization/folder2/folder1/file2.md","url":"/3DV/Localization/folder2/folder1/file2.html"},{"layout":"default","title":"file3","content":"# file3\n\nsource: `{{ page.path }}`\n","dir":"/3DV/Localization/folder2/folder1/","name":"file3.md","path":"3DV/Localization/folder2/folder1/file3.md","url":"/3DV/Localization/folder2/folder1/file3.html"},{"permalink":"/3DV/Mapping/","layout":"default","title":"Mapping","content":"# Mapping\n\nsource: `{{ page.path }}`\n","dir":"/3DV/Mapping/","name":"README.md","path":"3DV/Mapping/README.md","url":"/3DV/Mapping/"},{"layout":"default","title":"file1","content":"# file1\n\nsource: `{{ page.path }}`\n","dir":"/3DV/Mapping/","name":"file1.md","path":"3DV/Mapping/file1.md","url":"/3DV/Mapping/file1.html"},{"layout":"default","title":"file2","content":"# file2\n\nsource: `{{ page.path }}`\n","dir":"/3DV/Mapping/","name":"file2.md","path":"3DV/Mapping/file2.md","url":"/3DV/Mapping/file2.html"},{"layout":"default","title":"file3","content":"# file3\n\nsource: `{{ page.path }}`\n","dir":"/3DV/Mapping/","name":"file3.md","path":"3DV/Mapping/file3.md","url":"/3DV/Mapping/file3.html"},{"permalink":"/3DV/Mapping/folder1/","layout":"default","title":"I’m folder1","content":"# I'm folder1\n\nsource: `{{ page.path }}`\n","dir":"/3DV/Mapping/folder1/","name":"README.md","path":"3DV/Mapping/folder1/README.md","url":"/3DV/Mapping/folder1/"},{"layout":"default","title":"file1","content":"# file1\n\nsource: `{{ page.path }}`\n","dir":"/3DV/Mapping/folder1/","name":"file1.md","path":"3DV/Mapping/folder1/file1.md","url":"/3DV/Mapping/folder1/file1.html"},{"layout":"default","title":"file2","content":"# file2\n\nsource: `{{ page.path }}`\n","dir":"/3DV/Mapping/folder1/","name":"file2.md","path":"3DV/Mapping/folder1/file2.md","url":"/3DV/Mapping/folder1/file2.html"},{"layout":"default","title":"file3","content":"# file3\n\nsource: `{{ page.path }}`\n","dir":"/3DV/Mapping/folder1/","name":"file3.md","path":"3DV/Mapping/folder1/file3.md","url":"/3DV/Mapping/folder1/file3.html"},{"permalink":"//","layout":"default","title":"3D Vision and Geometry","content":"# 3D Vision and Geometry\n\nThis website is designed for paper sharing and discussion at the VCL lab in Peking University.\n\n\n## Topics\n\n- Shape Generation\n- Geometry Feature Learning\n- Single View Reconstruction\n- Neural 3D Renderer\n- Shape Analysis","dir":"/","name":"README.md","path":"README.md","url":"/"},{"permalink":"/3DV/Localization/","layout":"default","title":"Localization","content":"# Localization\n\nsource: `{{ page.path }}`\n","dir":"/3DV/Localization/","name":"README.md","path":"3DV/Localization/README.md","url":"/3DV/Localization/"},{"sort":0,"permalink":"/Geometry/Shape-Editing-or-Deformation/","layout":"default","title":"Shape Deformation and Editing","content":"<h1 id=\"shape-deformation-and-editing\">Shape Deformation and Editing</h1>\n\n<p>Related works:</p>\n\n<ul>\n  <li><a href=\"/Geometry/Shape-Editing-or-Deformation/DeformSyncNet.html\">DeformSyncNet: Deformation Transfer via Synchronized Shape Deformation Spaces</a></li>\n</ul>\n","dir":"/Geometry/Shape-Editing-or-Deformation/","name":"README.md","path":"Geometry/Shape-Editing-or-Deformation/README.md","url":"/Geometry/Shape-Editing-or-Deformation/"},{"sort":1,"layout":"default","title":"PointCNN","content":"<h1 id=\"pointcnn\">PointCNN</h1>\n\n<p>Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen.</p>\n\n<p>NIPS 2018. <a href=\"https://github.com/yangyanli/PointCNN\">Github</a></p>\n\n","dir":"/Geometry/Geometry-Feature-Learning/","name":"PointCNN.md","path":"Geometry/Geometry-Feature-Learning/PointCNN.md","url":"/Geometry/Geometry-Feature-Learning/PointCNN.html"},{"sort":1,"layout":"default","title":"Learning Representations and Generative Models for 3D Point Clouds","content":"# Learning Representations and Generative Models for 3D Point Clouds\n\nPanos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, Leonidas Guibas\n\nICML 2018. [PDF][PDF]\n\n[PDF]:https://arxiv.org/abs/1707.02392","dir":"/Geometry/Shape-Generation/","name":"rawGAN.md","path":"Geometry/Shape-Generation/rawGAN.md","url":"/Geometry/Shape-Generation/rawGAN.html"},{"sort":1,"layout":"default","title":"DISN: Deep Implicit Surface Network for High-quality Single-view 3D Reconstruction","content":"<h1 id=\"disn-deep-implicit-surface-network-for-high-quality-single-view-3d-reconstruction\">DISN: Deep Implicit Surface Network for High-quality Single-view 3D Reconstruction</h1>\n\n<p>Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomir Mech, Ulrich Neumann</p>\n\n<p>NIPS 2019 <a href=\"https://arxiv.org/abs/1904.04290\">PDF</a>|<a href=\"https://github.com/laughtervv/DISN\">Git</a></p>\n\n<p><img src=\"./disn-result.png\" />\nReconstructing 3D shapes from single-view images has been a long-standing research problem. In this paper, we present DISN, a Deep Implicit Surface Network which can generate a high-quality detail-rich 3D mesh from a 2D image by predicting the underlying signed distance fields. In addition to utilizing global image features, DISN predicts the projected location for each 3D point on the 2D image and extracts local features from the image feature maps. Combining global and local features significantly improves the accuracy of the signed distance field prediction, especially for the detail-rich areas. To the best of our knowledge, DISN is the first method that constantly captures details such as holes and thin structures present in 3D shapes from single-view images. DISN achieves the state-of-the-art single-view reconstruction performance on a variety of shape categories reconstructed from both synthetic and real images.</p>\n\n<ul>\n  <li>\n    <h3 id=\"network\">Network</h3>\n    <p><img src=\"./disn-network.png\" /></p>\n  </li>\n</ul>\n\n","dir":"/Geometry/Single-View-Reconstruction/","name":"DISN.md","path":"Geometry/Single-View-Reconstruction/DISN.md","url":"/Geometry/Single-View-Reconstruction/DISN.html"},{"sort":1,"layout":"default","title":"Neural Rerendering in the Wild","content":"<h1 id=\"neural-rerendering-in-the-wild\">Neural Rerendering in the Wild</h1>\n\n<p>Moustafa Meshry, Dan B Goldman, Sameh Khamis, Hugues Hoppe, Rohit Pandey, Noah Snavely, Ricardo Martin-Brualla.</p>\n\n<p>CVPR 2019. <a href=\"https://arxiv.org/abs/1904.04290\">PDF</a></p>\n\n","dir":"/Geometry/Differentiable-Renderer/","name":"Neural-Rerendering-in-the-Wild.md","path":"Geometry/Differentiable-Renderer/Neural-Rerendering-in-the-Wild.md","url":"/Geometry/Differentiable-Renderer/Neural-Rerendering-in-the-Wild.html"},{"sort":1,"permalink":"/Geometry/","layout":"default","title":"Geometry","content":"<h1 id=\"geometry\">Geometry</h1>\n\n<ul>\n  <li><a href=\"/Geometry/Shape-Editing-or-Deformation/\">Shape Deformation and Editing</a>\n    <ul>\n      <li><a href=\"/Geometry/Shape-Editing-or-Deformation/DeformSyncNet.html\">DeformSyncNet: Deformation Transfer via Synchronized Shape Deformation Spaces</a></li>\n    </ul>\n  </li>\n  <li><a href=\"/Geometry/Shape-Generation/\">Shape Generation</a>\n    <ul>\n      <li><a href=\"/Geometry/Shape-Generation/rawGAN.html\">Learning Representations and Generative Models for 3D Point Clouds</a></li>\n      <li><a href=\"/Geometry/Shape-Generation/PolyGen.html\">PolyGen: An Autoregressive Generative Model of 3D Meshes</a></li>\n      <li><a href=\"/Geometry/Shape-Generation/ParSeNet.html\">ParSeNet: A Parametric Surface Fitting Network for 3D Point Clouds</a></li>\n    </ul>\n  </li>\n  <li><a href=\"/Geometry/Single-View-Reconstruction/\">Single View Reconstruction</a>\n    <ul>\n      <li><a href=\"/Geometry/Single-View-Reconstruction/DISN.html\">DISN: Deep Implicit Surface Network for High-quality Single-view 3D Reconstruction</a></li>\n      <li><a href=\"/Geometry/Single-View-Reconstruction/Pixel2Mesh.html\">Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images</a></li>\n    </ul>\n  </li>\n  <li><a href=\"/Geometry/Geometry-Feature-Learning/\">Geometry Feature Learning</a>\n    <ul>\n      <li><a href=\"/Geometry/Geometry-Feature-Learning/PointCNN.html\">PointCNN</a></li>\n      <li><a href=\"/Geometry/Geometry-Feature-Learning/PointNet.html\">PointNet</a></li>\n      <li><a href=\"/Geometry/Geometry-Feature-Learning/MeshCNN.html\">MeshCNN: A Network with an Edge</a></li>\n      <li><a href=\"/Geometry/Geometry-Feature-Learning/UV-Net.html\">UV-Net: Learning from Curve-Networks and Solids</a></li>\n    </ul>\n  </li>\n  <li><a href=\"/Geometry/Differentiable-Renderer/\">Differentiable Renderer</a>\n    <ul>\n      <li><a href=\"/Geometry/Differentiable-Renderer/Neural-Rerendering-in-the-Wild.html\">Neural Rerendering in the Wild</a></li>\n      <li><a href=\"/Geometry/Differentiable-Renderer/DFR-Differentiable-Function-Rendering-for-Learning-3D-Generation-from-Images.html\">DFR: Differentiable Function Rendering for Learning 3D Generation from Images</a></li>\n    </ul>\n  </li>\n</ul>\n","dir":"/Geometry/","name":"README.md","path":"Geometry/README.md","url":"/Geometry/"},{"sort":1,"layout":"default","title":"DeformSyncNet: Deformation Transfer via Synchronized Shape Deformation Spaces","content":"<h1 id=\"deformsyncnet-deformation-transfer-via-synchronized-shape-deformation-spaces\">DeformSyncNet: Deformation Transfer via Synchronized Shape Deformation Spaces</h1>\n\n<p>Minhyuk Sung<em>, Zhenyu Jiang</em>, Panos Achlioptas, Niloy J. Mitra, Leonidas J. Guibas (* equal contribution)</p>\n\n<p>SIGGRAPH Asia 2020. <a href=\"https://arxiv.org/abs/2009.01456\">PDF</a>|<a href=\"https://github.com/Steve-Tod/DeformSyncNet\">Git</a></p>\n\n<p><img src=\"./deformsync-teaser.png\" />\nShape deformation is an important component in any geometry processing toolbox. The goal is to enable intuitive deformations of single or multiple shapes or to transfer example deformations to new shapes while preserving the plausibility of the deformed shape(s). Existing approaches assume access to point-level or part-level correspondence or establish them in a preprocessing phase, thus limiting the scope and generality of such approaches. We propose DeformSyncNet, a new approach that allows consistent and synchronized shape deformations without requiring explicit correspondence information. Technically, we achieve this by encoding deformations into a class-specific idealized latent space while decoding them into an individual, model-specific linear deformation action space, operating directly in 3D. The underlying encoding and decoding are performed by specialized (jointly trained) neural networks. By design, the inductive bias of our networks results in a deformation space with several desirable properties, such as path invariance across different deformation pathways, which are then also approximately preserved in real space. We qualitatively and quantitatively evaluate our framework against multiple alternative approaches and demonstrate improved performance.</p>\n\n<ul>\n  <li>\n    <h3 id=\"network\">Network</h3>\n    <p><img src=\"./deformsync-network.png\" text-align=\"center\" width=\"400\" /></p>\n  </li>\n</ul>\n\n","dir":"/Geometry/Shape-Editing-or-Deformation/","name":"DeformSyncNet.md","path":"Geometry/Shape-Editing-or-Deformation/DeformSyncNet.md","url":"/Geometry/Shape-Editing-or-Deformation/DeformSyncNet.html"},{"sort":1,"permalink":"/Practice/Preliminary/","layout":"default","title":"Preliminary","content":"<h1 id=\"preliminary\">Preliminary</h1>\n\n<ul>\n  <li><a href=\"/Practice/Preliminary/Generate-a-shape-from-X.html\">Generate a Shape from X</a></li>\n  <li><a href=\"/Practice/Preliminary/Shape-editing.html\">Shape Editing</a></li>\n  <li><a href=\"/Practice/Preliminary/Shape-analysis.html\">Shape Analysis</a></li>\n  <li><a href=\"/Practice/Preliminary/Special-topics-on-Face-Body-and-Facades.html\">Special Topics on Face, Body and Facades</a></li>\n</ul>\n","dir":"/Practice/Preliminary/","name":"README.md","path":"Practice/Preliminary/README.md","url":"/Practice/Preliminary/"},{"sort":1,"layout":"default","title":"Generate a Shape from X","content":"<h1 id=\"generate-a-shape-from-x\">Generate a Shape from X</h1>\n\n<h3 id=\"1-introduction-to-3d-digital-content\">1. Introduction to 3D digital content.</h3>\n\n<h3 id=\"2-how-to-represent-3d-shape-in-a-digital-platform\">2. How to represent 3D shape in a digital platform?</h3>\n\n<h3 id=\"3-rendering-3d-geometry-to-2d-media-and-browsing-with-local-or-internet-graphics-engine\">3. Rendering 3D geometry to 2D media and browsing with local or internet graphics engine.</h3>\n\n<ul>\n  <li>\n    <p>Opengl : understand the rasterization and shading process</p>\n  </li>\n  <li>\n    <p>Raytracing /&amp; raycasting: understand how to get photorealistic image by traveling with lights.</p>\n  </li>\n</ul>\n\n<h3 id=\"4-aquiring-3d-content-using-lidar-equipment\">4. Aquiring 3D content using Lidar equipment.</h3>\n\n<h3 id=\"5-approximate-3d-content-with-a-parametric-function\">5. Approximate 3D content with a parametric function.</h3>\n\n<p>A simple shape autoencoder. Two examples:</p>\n<ul>\n  <li>PointNet AE and</li>\n  <li>3D GAN.</li>\n</ul>\n\n<h3 id=\"6-generate-3d-shape-from-images\">6. Generate 3D shape from images.</h3>\n\n<p>Modeling joint distribution of 3D and image data collections.\nExamples:</p>\n<ul>\n  <li>Disn</li>\n  <li>Multiview</li>\n</ul>\n\n<h3 id=\"7-generate-3d-shape-from-abstract-design\">7. Generate 3D shape from abstract design.</h3>\n\n<p>Sketch to shape; text to shape; audio to shape…</p>\n\n<p>Examples of sketch to shape:</p>\n<ul>\n  <li>SketchCNN</li>\n  <li>DeepSketch</li>\n</ul>\n","dir":"/Practice/Preliminary/","name":"Generate-a-shape-from-X.md","path":"Practice/Preliminary/Generate-a-shape-from-X.md","url":"/Practice/Preliminary/Generate-a-shape-from-X.html"},{"sort":1,"permalink":"/Geometry/Shape-Generation/","layout":"default","title":"Shape Generation","content":"<h1 id=\"shape-generation\">Shape Generation</h1>\n\n<p>Related works:</p>\n\n<ul>\n  <li><a href=\"/Geometry/Shape-Generation/rawGAN.html\">Learning Representations and Generative Models for 3D Point Clouds</a></li>\n  <li><a href=\"/Geometry/Shape-Generation/PolyGen.html\">PolyGen: An Autoregressive Generative Model of 3D Meshes</a></li>\n  <li><a href=\"/Geometry/Shape-Generation/ParSeNet.html\">ParSeNet: A Parametric Surface Fitting Network for 3D Point Clouds</a></li>\n</ul>\n","dir":"/Geometry/Shape-Generation/","name":"README.md","path":"Geometry/Shape-Generation/README.md","url":"/Geometry/Shape-Generation/"},{"sort":2,"permalink":"/3DV/","layout":"default","title":"3D Vision","content":"<h1 id=\"3d-vision\">3D Vision</h1>\n\n<ul>\n  <li><a href=\"/3DV/Mapping/\">Mapping</a>\n    <ul>\n      <li><a href=\"/3DV/Mapping/file1.html\">file1</a></li>\n      <li><a href=\"/3DV/Mapping/file2.html\">file2</a></li>\n      <li><a href=\"/3DV/Mapping/file3.html\">file3</a></li>\n      <li><a href=\"/3DV/Mapping/folder1/\">I’m folder1</a>\n        <ul>\n          <li><a href=\"/3DV/Mapping/folder1/file1.html\">file1</a></li>\n          <li><a href=\"/3DV/Mapping/folder1/file2.html\">file2</a></li>\n          <li><a href=\"/3DV/Mapping/folder1/file3.html\">file3</a></li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li><a href=\"/3DV/Localization/\">Localization</a>\n    <ul>\n      <li><a href=\"/3DV/Localization/file1.html\">file1</a></li>\n      <li><a href=\"/3DV/Localization/file2.html\">file2</a></li>\n      <li><a href=\"/3DV/Localization/file3.html\">file3</a></li>\n      <li><a href=\"/3DV/Localization/folder2/\">I’m folder2</a>\n        <ul>\n          <li><a href=\"/3DV/Localization/folder2/file1.html\">file1</a></li>\n          <li><a href=\"/3DV/Localization/folder2/file2.html\">file2</a></li>\n          <li><a href=\"/3DV/Localization/folder2/file3.html\">file3</a></li>\n          <li><a href=\"/3DV/Localization/folder2/folder1/\">I’m folder1</a>\n            <ul>\n              <li><a href=\"/3DV/Localization/folder2/folder1/file1.html\">file1</a></li>\n              <li><a href=\"/3DV/Localization/folder2/folder1/file2.html\">file2</a></li>\n              <li><a href=\"/3DV/Localization/folder2/folder1/file3.html\">file3</a></li>\n            </ul>\n          </li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ul>\n","dir":"/3DV/","name":"README.md","path":"3DV/README.md","url":"/3DV/"},{"sort":2,"layout":"default","title":"PointNet","content":"<h1 id=\"pointnet\">PointNet</h1>\n\n<p>Charles R. Qi, Hao Su, Kaichun Mo, Leonidas J. Guibas</p>\n\n<p>CVPR 2017. <a href=\"https://github.com/charlesq34/pointnet\">Github</a></p>\n\n","dir":"/Geometry/Geometry-Feature-Learning/","name":"PointNet.md","path":"Geometry/Geometry-Feature-Learning/PointNet.md","url":"/Geometry/Geometry-Feature-Learning/PointNet.html"},{"sort":2,"layout":"default","title":"PolyGen: An Autoregressive Generative Model of 3D Meshes","content":"<h1 id=\"polygen-an-autoregressive-generative-model-of-3d-meshes\">PolyGen: An Autoregressive Generative Model of 3D Meshes</h1>\n\n<p>Charlie Nash, Yaroslav Ganin, S. M. Ali Eslami, Peter W. Battaglia (Google DeepMind)</p>\n\n<p><a href=\"https://arxiv.org/abs/2002.10880\">arxiv 2002.10880</a></p>\n\n<p>Generating n-gon mesh by using transformer to predict vertices and then the connecting edges.</p>\n\n<p>Major limitation: do not guarantee each face to be planar!</p>\n","dir":"/Geometry/Shape-Generation/","name":"PolyGen.md","path":"Geometry/Shape-Generation/PolyGen.md","url":"/Geometry/Shape-Generation/PolyGen.html"},{"sort":2,"layout":"default","title":"Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images","content":"<h1 id=\"pixel2mesh-generating-3d-mesh-models-from-single-rgb-images\">Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images</h1>\n\n<p>Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, Yu-Gang Jiang</p>\n\n<p>ECCV 2018. <a href=\"https://arxiv.org/abs/1804.01654\">PDF</a></p>\n\n","dir":"/Geometry/Single-View-Reconstruction/","name":"Pixel2Mesh.md","path":"Geometry/Single-View-Reconstruction/Pixel2Mesh.md","url":"/Geometry/Single-View-Reconstruction/Pixel2Mesh.html"},{"sort":2,"permalink":"/Geometry/Single-View-Reconstruction/","layout":"default","title":"Single View Reconstruction","content":"<h1 id=\"single-view-reconstruction\">Single View Reconstruction</h1>\n\n<p>Related works:</p>\n\n<ul>\n  <li><a href=\"/Geometry/Single-View-Reconstruction/DISN.html\">DISN: Deep Implicit Surface Network for High-quality Single-view 3D Reconstruction</a></li>\n  <li><a href=\"/Geometry/Single-View-Reconstruction/Pixel2Mesh.html\">Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images</a></li>\n</ul>\n","dir":"/Geometry/Single-View-Reconstruction/","name":"README.md","path":"Geometry/Single-View-Reconstruction/README.md","url":"/Geometry/Single-View-Reconstruction/"},{"sort":2,"permalink":"/Practice/Advance/","layout":"default","title":"Advance","content":"<h1 id=\"advance\">Advance</h1>\n\n","dir":"/Practice/Advance/","name":"README.md","path":"Practice/Advance/README.md","url":"/Practice/Advance/"},{"sort":2,"layout":"default","title":"Shape Editing","content":"<h1 id=\"shape-editing\">Shape Editing</h1>\n\n<h3 id=\"1-how-to-represent-3d-shape-in-the-riemann-space\">1. How to represent 3D shape in the Riemann space?</h3>\n\n<p>Euclidean space versus Manifold space</p>\n\n<p>Local coordinate system: Laplacian coordinate</p>\n\n<h3 id=\"2-a-few-notations-of-differential-geometry-and-its-explanation\">2. A few notations of differential geometry and its explanation</h3>\n\n<h3 id=\"3-deformation-matrix-and-the-optimization-function\">3. Deformation matrix and the optimization function</h3>\n\n<p>Laplacian shape editing</p>\n\n<h3 id=\"4-shape-editing-in-latent-space\">4. Shape editing in latent space.</h3>\n\n<p>represent shape with a latent code, and deform the shape by moving the latent code to a new position in the latent space.</p>\n\n<p>Examples:</p>\n<ul>\n  <li>DeformSyncNet</li>\n  <li>DualSDF</li>\n</ul>\n\n","dir":"/Practice/Preliminary/","name":"Shape-editing.md","path":"Practice/Preliminary/Shape-editing.md","url":"/Practice/Preliminary/Shape-editing.html"},{"sort":2,"layout":"default","title":"DFR: Differentiable Function Rendering for Learning 3D Generation from Images","content":"<h1 id=\"dfr-differentiable-function-rendering-for-learning-3d-generation-from-images\">DFR: Differentiable Function Rendering for Learning 3D Generation from Images</h1>\n\n<p>Jiahui Lyu   Bojian Wu   Dani Lischinski   Daniel Cohen-Or   Hui Huang</p>\n\n<p>ACM Transactions on Graphics (Proceedings of SIGGRAPH ASIA 2020)</p>\n\n<p><a href=\"https://vcc.tech/research/2020/DRT\">Project</a></p>\n\n","dir":"/Geometry/Differentiable-Renderer/","name":"DFR-Differentiable-Function-Rendering-for-Learning-3D-Generation-from-Images.md","path":"Geometry/Differentiable-Renderer/DFR-Differentiable-Function-Rendering-for-Learning-3D-Generation-from-Images.md","url":"/Geometry/Differentiable-Renderer/DFR-Differentiable-Function-Rendering-for-Learning-3D-Generation-from-Images.html"},{"sort":3,"layout":"default","title":"ParSeNet: A Parametric Surface Fitting Network for 3D Point Clouds","content":"<h1 id=\"parsenet-a-parametric-surface-fitting-network-for-3d-point-clouds\">ParSeNet: A Parametric Surface Fitting Network for 3D Point Clouds</h1>\n\n<p>Gopal Sharma, Difan Liu, Evangelos Kalogerakis, Subhransu Maji, Siddhartha Chaudhuri, Radomír Měch</p>\n\n<p><a href=\"https://hippogriff.github.io/parsenet/\">ECCV 2020</a></p>\n\n<p>Decompose a 3D point cloud into paramatric surface patches (including B-spline patches).</p>\n\n<p>Major limitation: predicted surface patches are infinite, i.e. do not predict surface boundraies explicitly.</p>\n","dir":"/Geometry/Shape-Generation/","name":"ParSeNet.md","path":"Geometry/Shape-Generation/ParSeNet.md","url":"/Geometry/Shape-Generation/ParSeNet.html"},{"sort":3,"layout":"default","title":"Shape Analysis","content":"<h1 id=\"shape-analysis\">Shape Analysis</h1>\n\n<h3 id=\"1-more-notations-of-differential-geometry-and-its-explanation\">1. More notations of differential geometry and its explanation</h3>\n\n<h3 id=\"1-saliency-detection\">1. Saliency detection</h3>\n\n<h3 id=\"2-segmentation\">2. Segmentation</h3>\n\n<h3 id=\"3-symmetry-detection\">3. Symmetry detection</h3>\n\n<h3 id=\"4-shape-grammar\">4. Shape Grammar</h3>\n\n<p>E.g., procedural modeling, and the inverse process.</p>\n","dir":"/Practice/Preliminary/","name":"Shape-analysis.md","path":"Practice/Preliminary/Shape-analysis.md","url":"/Practice/Preliminary/Shape-analysis.html"},{"sort":3,"permalink":"/Geometry/Geometry-Feature-Learning/","layout":"default","title":"Geometry Feature Learning","content":"<h1 id=\"geometry-feature-learning\">Geometry Feature Learning</h1>\n\n<p>Related works:</p>\n\n<ul>\n  <li><a href=\"/Geometry/Geometry-Feature-Learning/PointCNN.html\">PointCNN</a></li>\n  <li><a href=\"/Geometry/Geometry-Feature-Learning/PointNet.html\">PointNet</a></li>\n  <li><a href=\"/Geometry/Geometry-Feature-Learning/MeshCNN.html\">MeshCNN: A Network with an Edge</a></li>\n  <li><a href=\"/Geometry/Geometry-Feature-Learning/UV-Net.html\">UV-Net: Learning from Curve-Networks and Solids</a></li>\n</ul>\n","dir":"/Geometry/Geometry-Feature-Learning/","name":"README.md","path":"Geometry/Geometry-Feature-Learning/README.md","url":"/Geometry/Geometry-Feature-Learning/"},{"sort":3,"permalink":"/Practice/","layout":"default","title":"3D Programming","content":"<h1 id=\"3d-programming\">3D Programming</h1>\n\n<ul>\n  <li><a href=\"/Practice/Preliminary/\">Preliminary</a>\n    <ul>\n      <li><a href=\"/Practice/Preliminary/Generate-a-shape-from-X.html\">Generate a Shape from X</a></li>\n      <li><a href=\"/Practice/Preliminary/Shape-editing.html\">Shape Editing</a></li>\n      <li><a href=\"/Practice/Preliminary/Shape-analysis.html\">Shape Analysis</a></li>\n      <li><a href=\"/Practice/Preliminary/Special-topics-on-Face-Body-and-Facades.html\">Special Topics on Face, Body and Facades</a></li>\n    </ul>\n  </li>\n  <li><a href=\"/Practice/Advance/\">Advance</a></li>\n</ul>\n","dir":"/Practice/","name":"README.md","path":"Practice/README.md","url":"/Practice/"},{"sort":3,"layout":"default","title":"MeshCNN: A Network with an Edge","content":"<h1 id=\"meshcnn-a-network-with-an-edge\">MeshCNN: A Network with an Edge</h1>\n\n<p><a href=\"https://www.cs.tau.ac.il/~hanocka/\">Rana Hanocka</a>, <a href=\"http://pxcm.org/\">Amir Hertz</a>, <a href=\"https://www.cs.tau.ac.il/~noafish/\">Noa Fish</a>, <a href=\"http://web.eng.tau.ac.il/~raja\">Raja Giryes</a>, <a href=\"https://scholar.google.co.il/citations?user=nEpKS-8AAAAJ&amp;hl=en\">Shachar Fleishman</a> and <a href=\"https://www.cs.tau.ac.il/~dcor/\">Daniel Cohen-Or</a></p>\n\n<p><a href=\"https://ranahanocka.github.io/MeshCNN/\">SIGGRAPH 2019</a></p>\n\n<p>Define edge-based feature on mesh to enable convolution operation and use edge collapse to minic pooling operation.</p>\n\n","dir":"/Geometry/Geometry-Feature-Learning/","name":"MeshCNN.md","path":"Geometry/Geometry-Feature-Learning/MeshCNN.md","url":"/Geometry/Geometry-Feature-Learning/MeshCNN.html"},{"sort":4,"permalink":"/Geometry/Differentiable-Renderer/","layout":"default","title":"Differentiable Renderer","content":"<h1 id=\"differentiable-renderer\">Differentiable Renderer</h1>\n\n<p>Related works:</p>\n\n<ul>\n  <li><a href=\"/Geometry/Differentiable-Renderer/Neural-Rerendering-in-the-Wild.html\">Neural Rerendering in the Wild</a></li>\n  <li><a href=\"/Geometry/Differentiable-Renderer/DFR-Differentiable-Function-Rendering-for-Learning-3D-Generation-from-Images.html\">DFR: Differentiable Function Rendering for Learning 3D Generation from Images</a></li>\n</ul>\n","dir":"/Geometry/Differentiable-Renderer/","name":"README.md","path":"Geometry/Differentiable-Renderer/README.md","url":"/Geometry/Differentiable-Renderer/"},{"sort":4,"layout":"default","title":"Special Topics on Face, Body and Facades","content":"<h1 id=\"special-topics-on-face-body-and-facades\">Special Topics on Face, Body and Facades</h1>\n\n<h3 id=\"1-performance-capturing-and-animation\">1. Performance capturing and animation</h3>\n\n<h3 id=\"2-dynamic-body-fusion-and-animation\">2. Dynamic body fusion and animation</h3>\n\n<h3 id=\"3-facades-modeling\">3. Facades modeling</h3>\n\n","dir":"/Practice/Preliminary/","name":"Special-topics-on-Face-Body-and-Facades.md","path":"Practice/Preliminary/Special-topics-on-Face-Body-and-Facades.md","url":"/Practice/Preliminary/Special-topics-on-Face-Body-and-Facades.html"},{"sort":4,"layout":"default","title":"UV-Net: Learning from Curve-Networks and Solids","content":"<h1 id=\"uv-net-learning-from-curve-networks-and-solids\">UV-Net: Learning from Curve-Networks and Solids</h1>\n\n<p>Pradeep Kumar Jayaraman, Aditya Sanghi, Joseph Lambourne, Thomas Davies, Hooman Shayani, Nigel Morris</p>\n\n<p><a href=\"https://arxiv.org/abs/2006.10211\">arxiv 2006.10211</a></p>\n\n<p>A encoder for boundary representation by taking face-adjacent graph as topology and extracting face feature via UV mapping.</p>\n\n","dir":"/Geometry/Geometry-Feature-Learning/","name":"UV-Net.md","path":"Geometry/Geometry-Feature-Learning/UV-Net.md","url":"/Geometry/Geometry-Feature-Learning/UV-Net.html"}]